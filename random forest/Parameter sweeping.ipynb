{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter sweeping\n",
    "\n",
    "This notebook talks about parameter sweeping for random forest classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters in random forest are either to increase the predictive power of the model or to make it easier to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Features which make predictions of the model better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are primarily 3 features which can be tuned to improve the predictive power of the model:\n",
    "### 1.1 max_features :\n",
    "The number of features to consider when looking for the best split.  \n",
    "There are multiple options available to assign maximum features. Common choices including:<br>\n",
    "Auto: max_features=sqrt(n_features)<br>\n",
    "None: max_features=n_features<br>\n",
    "float: max_features is a percentage and int(max_features * n_features) features are considered at each split<br>\n",
    "Increasing max_features generally improves the performance of the model as at each node now we have a higher number of options to be considered. But you decrease the speed of algorithm by doing so for sure. \n",
    "### 1.2 n_estimators：\n",
    "This is the number of trees you want to build. default=10 <br>\n",
    "Higher number of trees give you better performance but makes your code slower. You should choose as high value as your processor can handle because this makes your predictions stronger and more stable.\n",
    "### 1.3 min_sample_leaf :\n",
    "The minimum number of samples required to be at a leaf node。\n",
    "A smaller leaf makes the model more prone to capturing noise in train data. \n",
    "You should try multiple leaf sizes to find the most optimum for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Features which will make the model training easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few attributes which have a direct impact on model training speed. Following are the key parameters which you can tune for model speed :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 n_jobs:\n",
    "The number of jobs to run in parallel for both fit and predict.\n",
    "A value of “-1” means there is no restriction whereas a value of “1” means it can only use one processor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  random_state :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This parameter makes a solution easy to replicate. A definite value of random_state will always produce same results if given with same parameters and training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 oob_score :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether to use out-of-bag samples to estimate the generalization accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter sweeping for a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)# display all the columns\n",
    "raw_data = pd.read_csv('a_20s_1600her_0.4__maf_0.2_EDM-2_01.txt', sep = \"\\t\")# read in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = raw_data.iloc[:, -1].values\n",
    "X = raw_data.iloc[:, :-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "# This function does 10-fold. It saves the result at each time as different parts of y_pred. \n",
    "# In the end, it returns the y_pred as the result of all the 10-fold.\n",
    "def run_cv(X,y,clf_class,**kwargs):\n",
    "    # Construct a kfolds object\n",
    "    kf = KFold(len(y),n_folds=10,shuffle=True) # Total number of elements；Number of folds， default=3；Whether to shuffle the data before splitting into batches\n",
    "    y_pred = y.copy()\n",
    "    clf = clf_class(**kwargs)\n",
    "    # Iterate through folds\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        \n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function calculates accuracy\n",
    "def accuracy(y_true,y_pred):\n",
    "    return np.mean(y_true == y_pred) # NumPy interpretes True and False as 1. and 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_estimators_options = list(range(1, 100, 10))\n",
    "sample_leaf_options = list(range(1, 20, 5))\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for leaf_size in sample_leaf_options:\n",
    "    for n_estimators_size in n_estimators_options:\n",
    "        RF_CV_result = run_cv(X,y,RandomForestClassifier, min_samples_leaf=leaf_size, n_estimators=n_estimators_size, random_state=50,n_jobs = -1)\n",
    "        # Record current min_samples_leaf，n_estimators and accuracy\n",
    "        results.append((leaf_size, n_estimators_size, str(accuracy(y, RF_CV_result))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find result with highest accuracy\n",
    "def find_best(lst):\n",
    "    return max(lst, key=lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After parameter sweeping, we find:\n",
      "when min_samples_leaf = 6 , n_estimators = 71 , accuracy = 0.77625\n"
     ]
    }
   ],
   "source": [
    "print ('After parameter sweeping, we find:')\n",
    "print ('when min_samples_leaf =', find_best(results)[0], ', n_estimators =',find_best(results)[1], ', accuracy =',find_best(results)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "Parameter sweeping can improve model performance. Before parameter sweeping, the accuracy of Random Forest model in task 2 is 0.701875. After optimizing the value of two parameters, the accuracy increased to 0.77625. For a given dataset, we can do parameter sweeping to improve the predictive power. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
